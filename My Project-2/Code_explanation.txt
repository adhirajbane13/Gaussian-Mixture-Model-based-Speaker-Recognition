1. Training_model.py: First, we have to execute this python code for creating and training the model. At first, all the required modules and their built-in functions are imported. For using the modules, like Numpy, librosa, etc., they need to be installed in the computer using pip of the python version present in the computer. The source folder where the train data is stored is given along with the destination folder where the GMMs of each speaker would be saved. There is a text file named 'Traindatapath.txt' where the path of all the training data is provided. That file is opened and read. Now, executing the for loop, there are 3 files for each speaker which are loaded thus providing the audio signal and its sample rate. Now, using the user-defined function extract_features of feature_extr.py, the MFCC and delta-MFCC features stacked together as a 40-dimensional vector for each file is extracted. When feature extraction is done for all 3 files of a speaker, count=3 and within the if statement, the GMM is created for that speaker, by fitting the extracted features vector into the created GMM. Finally, the GMM object is dumped in a file in the destination folder and named as 'SpeakerName.gmm' and the count is made 0 for starting the whole process for the next speaker. This way by this code, we get trained models for each speaker to be used in test.py.

2. Test.py: Again, here same as before firstly we will import all the necessary modules and in-built functions. Then, we provide the source folder where the test data is stored and also the folder where all the trained models of each speaker is stored. We store all the gmm files in the variable gmm_files. Then, from the stored gmm files we load all the gmm trained models in the array called 'models'. ALso, from each gmm file, we extract each speaker's name and store it in speakers array. Now, we offer the user two choices whether the user wants to test a single file(press 1) or all the files(press 0). If '1' is pressed, the particular test audio file is requested and it is loaded using its total path directory to obtain the audio signal and the sample rate. Again, using the user-defined function extract_features of feature_extr.py, the MFCC and delta-MFCC features stacked together as a 40-dimensional vector of the file is extracted, which are tested by calculating its likelihood with each speaker's gmm trained model. For whichever speaker model, the likelihood will be maximum, that speaker would be chosen as the winner and the speaker's name will be provided as the detection result. If '0' is pressed, same process will be followed as before, the only difference being that all the files will be tested. To access all the files, a for loop is executed where the the loop object will be the file name extracted from the text file, "Test_Datapath.txt". For each iteration, the errpr and sample no. is calculated. At the end, the accuracy is measured using the error value calulated, i.e, how many speaker detections went wrong. Finally, in this case the test accuracy and no. of samples tested is printed on the screen.